{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Textanalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# The file format of an Amazon line is: \"id\",\"title\",\"description\",\"manufacturer\",\"price\"\n",
    "\n",
    "#The file format of a Google line is: \"id\",\"name\",\"description\",\"manufacturer\",\"price\"\n",
    "\n",
    "datafile_patern = \"^(.+),(.+),(.*),(.*),(.*)\"\n",
    "stopwords = set(sc.textFile('googleamazondata/stopwords.txt').collect())\n",
    "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
    "\n",
    "def removeQuotes(s):\n",
    "    \"\"\" Remove quotation marks from an input string\n",
    "    Args:\n",
    "        s (str): input string that might have the quote \"\" characters\n",
    "    Returns:\n",
    "        str: a string without the quote characters\n",
    "    \"\"\"\n",
    "    return ''.join(i for i in s if i!='\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsedatafileline(datafileline):\n",
    "    '''\n",
    "    Args:\n",
    "        datafileline (str): input string that is a line from data file\n",
    "    Returns:\n",
    "        str : a string parsed using regular expression and without punctuation\n",
    "    '''\n",
    "    \n",
    "    match = re.search(datafile_patern, datafileline)\n",
    "    \n",
    "    if match is None:\n",
    "        print('invalid datafileline: %s' %datafileline)\n",
    "        return(datafileline, -1)\n",
    "    elif match.group(1) == 'id':\n",
    "        print('Header datafile line: %s' % datafileline)\n",
    "        return(datafileline, 1)\n",
    "    else:\n",
    "        product = \"%s %s %s\" %(match.group(2), match.group(3),match.group(4))\n",
    "        return ((removeQuotes(match.group(1)), product),1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "baseDir = os.path.join('googleamazondata')\n",
    "\n",
    "GOOGLE_PATH = 'google.csv'\n",
    "GOOGLE_SMALL_PATH = 'googlesmall.csv'\n",
    "AMAZON_PATH = 'amazon.csv'\n",
    "AMAZON_SMALL_PATH = 'amazonsmall.csv'\n",
    "GOLD_STANDARD_PATH = 'mapping.csv'\n",
    "STOPWORDS_PATH = 'stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsedata(filename):\n",
    "    \n",
    "    return (sc\n",
    "            .textFile(filename, 4, 0)\n",
    "            .map(lambda x: x.decode('utf-8'))\n",
    "            .map(parsedatafileline)\n",
    "            .cache())\n",
    "\n",
    "def loadData(path):\n",
    "    filename = os.path.join(baseDir, path)\n",
    "    raw = parsedata(filename)\n",
    "    failed = (raw\n",
    "              .filter(lambda x: x[1] == -1)\n",
    "              .map(lambda x: x[0]))\n",
    "    for line in failed.take(10):\n",
    "        print('%s - invalid datafile line: %s'%(path, line))\n",
    "    valid = (raw\n",
    "             .filter(lambda x: x[1] == 1)\n",
    "             .map(lambda x: x[0])\n",
    "             .cache())\n",
    "    print ('%s - Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (path,\n",
    "                                                                                        raw.count(),\n",
    "                                                                                        valid.count(),\n",
    "                                                                                        failed.count()))\n",
    "    assert failed.count() == 0\n",
    "    assert raw.count() == valid.count()\n",
    "    \n",
    "    return valid\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googlesmall.csv - Read 201 lines, successfully parsed 201 lines, failed to parse 0 lines\n",
      "google.csv - Read 3227 lines, successfully parsed 3227 lines, failed to parse 0 lines\n",
      "amazonsmall.csv - Read 201 lines, successfully parsed 201 lines, failed to parse 0 lines\n",
      "amazon.csv - Read 1364 lines, successfully parsed 1364 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "googleSmall = loadData(GOOGLE_SMALL_PATH)\n",
    "google = loadData(GOOGLE_PATH)\n",
    "amazonSmall = loadData(AMAZON_SMALL_PATH)\n",
    "amazon = loadData(AMAZON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google: id: \"name\" \"description\" \"manufacturer\"\n",
      "\n",
      "google: http://www.google.com/base/feeds/snippets/11448761432933644608: \"spanish vocabulary builder\" \"expand your vocabulary! contains fun lessons that both teach and entertain you'll quickly find yourself mastering new terms. includes games and more!\" \n",
      "\n",
      "google: http://www.google.com/base/feeds/snippets/8175198959985911471: \"topics presents: museums of world\" \"5 cd-rom set. step behind the velvet rope to examine some of the most treasured collections of antiquities art and inventions. includes the following the louvre - virtual visit 25 rooms in full screen interactive video detailed map of the louvre ...\" \n",
      "\n",
      "amazon: id: \"title\" \"description\" \"manufacturer\"\n",
      "\n",
      "amazon: b000jz4hqo: \"clickart 950 000 - premier image pack (dvd-rom)\"  \"broderbund\"\n",
      "\n",
      "amazon: b0006zf55o: \"ca international - arcserve lap/desktop oem 30pk\" \"oem arcserve backup v11.1 win 30u for laptops and desktops\" \"computer associates\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine 3 lines from each small dataset\n",
    "for line in googleSmall.take(3):\n",
    "    print ('google: %s: %s\\n' % (line[0], line[1]))\n",
    "\n",
    "for line in amazonSmall.take(3):\n",
    "    print ('amazon: %s: %s\\n' % (line[0], line[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ER as Text Similarity - Bags of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean and tokenize a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removepunctuationtokenize(text, stopwords=None, split=False):\n",
    "    '''\n",
    "    Args : \n",
    "        text(str): input string that might contains punctuations\n",
    "    Returns:\n",
    "        str : a string clean of punctuation\n",
    "    '''\n",
    "    \n",
    "    regex = re.compile('[%s]'%string.punctuation)\n",
    "    text = regex.sub('', text.lower()).strip()\n",
    "    text = ''.join([x for x in text if ord(x)<208])\n",
    "    if split:\n",
    "        return [x for x in text.split() if x not in stopwords]\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazontoken = amazonSmall.map(lambda line: (line[0], removepunctuationtokenize(line[1],stopwords,split=True)))\n",
    "googletoken = googleSmall.map(lambda line: (line[0], removepunctuationtokenize(line[1], stopwords, split=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countTokens(vendorrdd):\n",
    "    return vendorrdd.map(lambda x: len(x[1])).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21525 tokens in the combined datasets\n"
     ]
    }
   ],
   "source": [
    "totalTokens = countTokens(amazontoken) + countTokens(googletoken)\n",
    "print('There are %s tokens in the combined datasets' % totalTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 5 Amazon records with the most tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 amazon records with most tokens are:\n",
      "\n",
      "Amazon ID b000o24l3q: 1487 tokens\n",
      "Amazon ID b000ndibtg: 1424 tokens\n",
      "Amazon ID b000i2qubi: 726 tokens\n",
      "Amazon ID b000ndicuy: 433 tokens\n",
      "Amazon ID b000h22pg8: 422 tokens\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 amazon records with most tokens are:\\n')\n",
    "for tup in amazontoken.map(lambda x: (x[0],len(x[1]))).takeOrdered(5, lambda x: -x[1]):\n",
    "    print('Amazon ID %s: %s tokens'%(tup[0],tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ER as Text Similarity - Weighted Bag-of-Words using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brown': 0.16666666666666666, 'fox': 0.16666666666666666, 'lazy': 0.16666666666666666, 'jumps': 0.16666666666666666, 'quick': 0.16666666666666666, 'dog': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "def tf(tokens):\n",
    "    \"\"\" Compute TF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "    Returns:\n",
    "        dictionary: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    total = len(tokens)\n",
    "    counts = {}\n",
    "    for token in tokens:\n",
    "        if token in counts:\n",
    "            counts[token] = counts[token] + 1\n",
    "        else:\n",
    "            counts[token] = 1\n",
    "    return {k: float(v) / total for k, v in counts.items()}\n",
    "\n",
    "print(tf(removepunctuationtokenize(quickbrownfox, stopwords, split=True))) # Should give { 'quick': 0.1666 ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a corpus\n",
    "corpusRdd = amazontoken.union(googletoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implement an IDFs function\n",
    "def idfs(corpus):\n",
    "    N = corpus.count()\n",
    "    uniquetokens = corpus.map(lambda x: set(x[1]))\n",
    "    tokencountpairtuple = uniquetokens.flatMap(lambda x: [(i,1) for i in x])\n",
    "    tokensumpairtuple   = tokencountpairtuple.reduceByKey(lambda a,b: a+b)\n",
    "    return(tokensumpairtuple.map(lambda x: (x[0], float(N)/x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfsSmall = idfs(corpusRdd)\n",
    "uniqueTokenCount = idfsSmall.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5095 unique tokens in the small datasets.\n"
     ]
    }
   ],
   "source": [
    "print('There are %s unique tokens in the small datasets.' % uniqueTokenCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('software', 4.276595744680851), ('features', 6.931034482758621), ('new', 7.052631578947368), ('complete', 7.3090909090909095), ('create', 8.375), ('windows', 8.73913043478261), ('system', 8.73913043478261), ('powerful', 9.571428571428571), ('use', 9.804878048780488), ('win', 9.804878048780488), ('tools', 10.05)]\n"
     ]
    }
   ],
   "source": [
    "#Tokens with the smallest IDF\n",
    "smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])\n",
    "print (smallIDFTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  20.,   63.,   78.,   64.,   62.,   92.,   71.,  107.,    0.,\n",
       "         139.,    0.,    0.,  247.,    0.,    0.,    0.,  398.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,  962.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0., 2792.]),\n",
       " array([  4.27659574,  12.23106383,  20.18553191,  28.14      ,\n",
       "         36.09446809,  44.04893617,  52.00340426,  59.95787234,\n",
       "         67.91234043,  75.86680851,  83.8212766 ,  91.77574468,\n",
       "         99.73021277, 107.68468085, 115.63914894, 123.59361702,\n",
       "        131.54808511, 139.50255319, 147.45702128, 155.41148936,\n",
       "        163.36595745, 171.32042553, 179.27489362, 187.2293617 ,\n",
       "        195.18382979, 203.13829787, 211.09276596, 219.04723404,\n",
       "        227.00170213, 234.95617021, 242.9106383 , 250.86510638,\n",
       "        258.81957447, 266.77404255, 274.72851064, 282.68297872,\n",
       "        290.63744681, 298.59191489, 306.54638298, 314.50085106,\n",
       "        322.45531915, 330.40978723, 338.36425532, 346.3187234 ,\n",
       "        354.27319149, 362.22765957, 370.18212766, 378.13659574,\n",
       "        386.09106383, 394.04553191, 402.        ]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x42b3c7ab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#IDF Histogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "small_idf_values = idfsSmall.map(lambda s: s[1]).collect()\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.hist(small_idf_values, 50, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement a TF-IDF function\n",
    "def tfidf(tokens, idfs):\n",
    "    tfs = tf(tokens)\n",
    "    tfIdfDict = {k:v*idfs[k] for k,v in tfs.items()}\n",
    "    return tfIdfDict\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon record \"b000hkgj8k\" has tokens and weights:\n",
      "{'courseware': 67.0, 'customizing': 16.75, 'autodesk': 8.375, 'interface': 3.0454545454545454, '2007': 3.722222222222222, 'psg': 33.5, 'autocad': 33.5}\n"
     ]
    }
   ],
   "source": [
    "recb000hkgj8k = amazontoken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\n",
    "idfsSmallWeights = idfsSmall.collectAsMap()\n",
    "rec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfsSmallWeights)\n",
    "\n",
    "print('Amazon record \"b000hkgj8k\" has tokens and weights:\\n%s' % rec_b000hkgj8k_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) ER as Text Similarity - Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.0 6.164414002968976 0.8262970212292282\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def dotprod(a, b):\n",
    "    \"\"\" Compute dot product\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        dotProd: result of the dot product with the two input dictionaries\n",
    "    \"\"\"\n",
    "    totalsum = 0.\n",
    "    for k1, v1 in a.items():\n",
    "        for k2, v2 in b.items():\n",
    "            if k1 == k2:\n",
    "                totalsum += v1 * v2\n",
    "    return totalsum\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product\n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value\n",
    "    Returns:\n",
    "        norm: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    return math.sqrt(dotprod(a, a))\n",
    "\n",
    "def cossim(a, b):\n",
    "    \"\"\" Compute cosine similarity\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        cossim: dot product of two dictionaries divided by the norm of the first dictionary and\n",
    "                then by the norm of the second dictionary\n",
    "    \"\"\"\n",
    "    return dotprod(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
    "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
    "dp = dotprod(testVec1, testVec2)\n",
    "nm = norm(testVec1)\n",
    "print (dp, nm,cossim(testVec1,testVec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a cosineSimilarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05772433821630338"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosineSimilarity(string1, string2, idfsDictionary):\n",
    "    \"\"\" Compute cosine similarity between two strings\n",
    "    Args:\n",
    "        string1 (str): first string\n",
    "        string2 (str): second string\n",
    "        idfsDictionary (dictionary): a dictionary of IDF values\n",
    "    Returns:\n",
    "        cossim: cosine similarity value\n",
    "    \"\"\"\n",
    "    w1 = tfidf(removepunctuationtokenize(string1, stopwords, split=True), idfsDictionary)\n",
    "    w2 = tfidf(removepunctuationtokenize(string2, stopwords, split=True), idfsDictionary)\n",
    "    return cossim(w1, w2)\n",
    "\n",
    "cossimAdobe = cosineSimilarity('Adobe Photoshop',\n",
    "                               'Adobe Illustrator',\n",
    "                               idfsSmallWeights)\n",
    "cossimAdobe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Entity Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossSmall = (googleSmall\n",
    "              .cartesian(amazonSmall)\n",
    "              .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSimilarity(record):\n",
    "    \"\"\" Compute similarity on a combination record\n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record)\n",
    "    Returns:\n",
    "        pair: a pair, (google URL, amazon ID, cosine similarity value)\n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[0]\n",
    "    \n",
    "    googleURL = googleRec[0]\n",
    "    amazonID = amazonRec[0]\n",
    "    \n",
    "    googleValue = googleRec[1]\n",
    "    amazonValue = amazonRec[1]\n",
    "    \n",
    "    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallWeights)\n",
    "    return (googleURL, amazonID, cs)\n",
    "\n",
    "similarities = (crossSmall\n",
    "                .map(lambda x: computeSimilarity(x))\n",
    "                .cache())\n",
    "\n",
    "def similar(amazonID, googleURL):\n",
    "    \"\"\" Return similarity value\n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL\n",
    "    Returns:\n",
    "        similar: cosine similarity value\n",
    "    \"\"\"\n",
    "    return (similarities\n",
    "            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n",
    "            .collect()[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested similarity is 0.0003207472319016042.\n"
     ]
    }
   ],
   "source": [
    "similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print ('Requested similarity is %s.' % similarityAmazonGoogle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Entity Resolution with Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested similarity is 0.0003207472319016042.\n"
     ]
    }
   ],
   "source": [
    "def computeSimilarityBroadcast(record):\n",
    "    \"\"\" Compute similarity on a combination record, using Broadcast variable\n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record)\n",
    "    Returns:\n",
    "        pair: a pair, (google URL, amazon ID, cosine similarity value)\n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    googleURL = googleRec[0]\n",
    "    amazonID = amazonRec[0]\n",
    "    googleValue = googleRec[1]\n",
    "    amazonValue = amazonRec[1]\n",
    "    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallBroadcast.value)\n",
    "    return (googleURL, amazonID, cs)\n",
    "\n",
    "idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)\n",
    "similaritiesBroadcast = (crossSmall\n",
    "                         .map(lambda x: computeSimilarityBroadcast(x))\n",
    "                         .cache())\n",
    "\n",
    "def similarBroadcast(amazonID, googleURL):\n",
    "    \"\"\" Return similarity value, computed using Broadcast variable\n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL\n",
    "    Returns:\n",
    "        similar: cosine similarity value\n",
    "    \"\"\"\n",
    "    return (similaritiesBroadcast\n",
    "            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n",
    "            .collect()[0][2])\n",
    "\n",
    "similarityAmazonGoogleBroadcast = similarBroadcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print ('Requested similarity is %s.' % similarityAmazonGoogleBroadcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a Gold Standard evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1301 lines, successfully parsed 1300 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "GOLDFILE_PATTERN = '^(.+),(.+)'\n",
    "\n",
    "def parse_goldfile_line(goldfile_line):\n",
    "    \n",
    "    \n",
    "    match = re.search(GOLDFILE_PATTERN, goldfile_line)\n",
    "    \n",
    "    if match is None:\n",
    "        print('Invalide Golfile line: %s'%goldfile_line)\n",
    "        return(goldfile_line, -1)\n",
    "    elif match.group(1) == '\"idAmazon\"':\n",
    "        print('The header is: %s'%goldfile_line)\n",
    "        return(goldfile_line, 0)\n",
    "    else:\n",
    "        key = '%s %s' % (removeQuotes(match.group(1)), removeQuotes(match.group(2)))\n",
    "        return ((key, 'gold'), 1)\n",
    "    \n",
    "    \n",
    "goldfile = os.path.join(baseDir, GOLD_STANDARD_PATH)\n",
    "gsRaw = (sc\n",
    "         .textFile(goldfile)\n",
    "         .map(parse_goldfile_line)\n",
    "         .cache())\n",
    "\n",
    "gsfailed = (gsRaw\n",
    "            .filter(lambda x: x[1] == -1)\n",
    "            .map(lambda x: x[0]))\n",
    "\n",
    "for line in gsfailed.take(10):\n",
    "    print('Invalid goldfile line: %s' % line)\n",
    "    \n",
    "\n",
    "goldStandard = (gsRaw\n",
    "                .filter(lambda s: s[1] == 1)\n",
    "                .map(lambda s: s[0])\n",
    "                .cache())\n",
    "\n",
    "print ('Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (gsRaw.count(),\n",
    "                                                                                 goldStandard.count(),\n",
    "                                                                                 gsfailed.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 true duplicates.\n",
      "The average similarity of true duplicates is 0.2555319924185273.\n",
      "And for non duplicates, it is 0.001055458878033001.\n"
     ]
    }
   ],
   "source": [
    "#Q: How many true duplicate pairs are there in the small datasets?\n",
    "#Q: What is the average similarity score for true duplicates?\n",
    "#Q: What about for non-duplicates? \n",
    "\n",
    "sims = similaritiesBroadcast.map(lambda x: (x[1] + ' ' + x[0], x[2]))\n",
    "\n",
    "trueDupsRDD = sims.join(goldStandard)\n",
    "trueDupsCount = trueDupsRDD.count()\n",
    "avgSimDups = trueDupsRDD.map(lambda x: x[1][0]).reduce(lambda a, b: a + b) / trueDupsCount\n",
    "\n",
    "trueDupsOrigRDD = trueDupsRDD.map(lambda x: (x[0],x[1][0]))\n",
    "nonDupsRDD = sims.subtract(trueDupsOrigRDD)\n",
    "avgSimNon = nonDupsRDD.map(lambda x: x[1]).reduce(lambda a, b: a + b) / nonDupsRDD.count()\n",
    "\n",
    "print ('There are %s true duplicates.' % trueDupsCount)\n",
    "print ('The average similarity of true duplicates is %s.' % avgSimDups)\n",
    "print ('And for non duplicates, it is %s.' % avgSimNon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
